nn_hidden_size = 512
NUM_OF_EPISODES = 5_000_000
num_envs = 1
num_steps_per_update = 2048

learning_rate = 0.0001
learning_rate_start_factor = 1.0
learning_rate_end_factor = 0.00001

#gae = true
#gamma = 0.99
#gae_lambda = 0.95

# the discount factor gamma
gamma = 0.99
# the number of mini-batches
num_minibatches = 4
# the K epochs to update the policy
update_epochs = 4
# Toggles advantages normalization
norm_adv = true
# the surrogate clipping coefficient
clip_coef = 0.2
# Toggles whether or not to use a clipped loss for the value function, as per the paper
clip_vloss = true
# coefficient of the entropy
ent_coef = 0.01
# coefficient of the value function
vf_coef = 0.5
# the maximum norm for the gradient clipping
max_grad_norm = 0.5
# the target KL divergence threshold
target_kl = 1e-3
